import math
from collections import Counter

def entropy(data):
    labels = [row[-1] for row in data]
    label_counts = Counter(labels)
    total = len(data)
    return -sum((count/total) * math.log2(count/total) for count in label_counts.values())

def info_gain(data, attr_index):
    total_entropy = entropy(data)
    attr_values = set(row[attr_index] for row in data)
    subset_entropy = 0
    for value in attr_values:
        subset = [row for row in data if row[attr_index] == value]
        weight = len(subset) / len(data)
        subset_entropy += weight * entropy(subset)
    return total_entropy - subset_entropy

def majority_class(data):
    labels = [row[-1] for row in data]
    return Counter(labels).most_common(1)[0][0]

def build_tree(data, attributes):
    labels = [row[-1] for row in data]
    if labels.count(labels[0]) == len(labels):
        return labels[0]
    if not attributes:
        return majority_class(data)

    gains = [info_gain(data, i) for i in range(len(attributes))]
    best_attr = gains.index(max(gains))
    best_attr_name = attributes[best_attr]

    tree = {best_attr_name: {}}
    values = set(row[best_attr] for row in data)

    for val in values:
        subset = [row[:best_attr] + row[best_attr+1:] for row in data if row[best_attr] == val]
        sub_attrs = attributes[:best_attr] + attributes[best_attr+1:]
        tree[best_attr_name][val] = build_tree(subset, sub_attrs)

    return tree

def print_tree(tree, indent=""):
    if isinstance(tree, str):
        print(indent + "=> " + tree)
    else:
        for key, branches in tree.items():
            for val, subtree in branches.items():
                print(f"{indent}{key} = {val}:")
                print_tree(subtree, indent + "  ")

# Build and display tree
decision_tree = build_tree(dataset, attributes)
print("Decision Tree:")
print_tree(decision_tree)
